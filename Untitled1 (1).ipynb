{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Classification Predict - AE6\n",
    "\n",
    "# Introduction\n",
    "Climate change through the lens of twitter.\n",
    "Given the recent explosion of Big Data, there is a growing demand for analyzing non traditional data sources. Social Media data is a huge source of this data in the form of chats, messages, news feeds and all of it is in an unstructured form. Text analytics is a process that helps analyze this unstructured data and look for patterns or infer popular sentiment which can help organizations in their decision making.\n",
    "\n",
    "Twitter data is a powerful source of information on a wide list of topics. This data can be analyzed to find trends related to specific topics, measure popular sentiment, obtain feedback on past decisions and also help make future decisions. Climate change has received extensive attention from public opinion in the last couple of years, after being considered for decades as an exclusive scientific debate. Governments and world-wide organizations such as the United Nations are working harder than ever on raising and maintaining public awareness toward this global issue.\n",
    "\n",
    "The aim of this project is to gauge the public perception of climate change using twitter data. This will allow companies access to a broad base of consumer sentiment, spanning multiple demographics and geographic categories - Thus increasing their insights and informing future marketing strategies.\n",
    "\n",
    "## Problem statement\n",
    "Increase Thrive Market’s advertising efficiency by using machine learning to create effective marketing tools that can identify whether or not a person believes in climate change and could possibly be converted to a new customer based on their tweets.\n",
    "\n",
    "## Table of contents:\n",
    "- Import libraries and datasets\n",
    "- First glance at the raw data\n",
    "- Preprocessing\n",
    "  * Create copy\n",
    "  * Hashtag extraction\n",
    "  * Tweet cleaning\n",
    "  * Parts of speech tagging and lemmatization\n",
    "  * Word frequency\n",
    "  * Specific named entity extraction\n",
    "- Exploratory data analysis\n",
    "  * Target variable distribution\n",
    "  * Tweet length distribution\n",
    "  * Climate change buzzwords\n",
    "  * Hashtags\n",
    "  * People, places and organizations to watch\n",
    "- Building classification models\n",
    "  * Train-validation split\n",
    "  * Pipelines\n",
    "  * Train models\n",
    "- Model evaluation\n",
    "  * Random forest\n",
    "  * Naive Bayes\n",
    "  * K nearest neighbors\n",
    "  * Logistic regression\n",
    "  * Linear SVC\n",
    "- Model Selection\n",
    "- Hyperparameter tuning\n",
    "- Submission\n",
    "- Conclusion\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Comet experiment¶\n",
    "We will be using Comet as a form of version control throughout the development of our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "!pip install comet_ml\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Setting the API key (saved as environment variable)\n",
    "experiment = Experiment(api_key=\"THysD8zqvW8wCiFTidV67jLP2\",\n",
    "                        project_name=\"climate-change-belief-analysis\", \n",
    "                        workspace=\"jamakasilwane\")                      \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and datasets¶\n",
    "First we need to load the libraries we are going to use throughout our notebook. After which we will load our train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Style\n",
    "import matplotlib.style as style \n",
    "sns.set(font_scale=1.5)\n",
    "style.use('seaborn-pastel')\n",
    "style.use('seaborn-poster')\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Downloads\n",
    "nlp = spacy.load('en')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Preprocessing\n",
    "import en_core_web_sm\n",
    "from collections import Counter\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords, wordnet  \n",
    "from sklearn.feature_extraction.text import CountVectorizer   \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "# Building classification models\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset \n",
    "train = pd.read_csv('../input/climate-change-belief-analysis/train.csv')\n",
    "test = pd.read_csv('../input/climate-change-belief-analysis/test.csv')\n",
    "sample = pd.read_csv('../input/climate-change-belief-analysis/sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First glance at the raw data¶\n",
    "The test and train data contain more than 10000 tweets... That's a lot of words!\n",
    "\n",
    "The tweets are divided into 4 classes:\n",
    "\n",
    "[ 2 ] News : Tweets linked to factual news about climate change.\n",
    "\n",
    "[ 1 ] Pro : Tweets that support the belief of man-made climate change.\n",
    "\n",
    "[ 0 ] Neutral : Tweets that neither support nor refuse beliefs of climate change.\n",
    "\n",
    "[-1 ] Anti : Tweets that do not support the belief of man-made climate change.\n",
    "\n",
    "Retweets account for 10% of the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking general look at both datasets\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "\n",
    "percent_duplicates = round((1-(train['message'].nunique()/len(train['message'])))*100,2)\n",
    "print('Duplicated tweets in train data:')\n",
    "print(percent_duplicates,'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing¶\n",
    "Before we continue exploring our data we will have to do some preprocessing in order to gain maximum insights.\n",
    "\n",
    "Plan of action:\n",
    "- Copy the dataframe and rename the class labels for better data visualization\n",
    "- Extract hashtags and store them in separate dataframes for each class\n",
    "- Remove 'noisy entities' such as URL's, punctuations, mentions, numbers and extra white space.\n",
    "- Tokenization\n",
    "- Perform part of speech tagging (POS) and lemmatization\n",
    "- Create dataframes to store the top 25 words and their respective frequencies in each class\n",
    "- Specific named entity extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create copy\n",
    "The first step in the preprocessing is to create a copy of the train dataframe for the EDA. The original dataframe will therefore be preserved. We proceed to rename the classes, converting the labels from numbers to the words they represent. This will make creating visuals with appropriate labels easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function creates a copy of the original train data and \n",
    "    renames the classes, converting them from numbers to words\n",
    "    \n",
    "    Input: \n",
    "    df: original dataframe\n",
    "        datatype: dataframe\n",
    "    \n",
    "    Output:\n",
    "    df: modified dataframe\n",
    "        datatype: dataframe \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    df = train.copy()\n",
    "    sentiment = df['sentiment']\n",
    "    word_sentiment = []\n",
    "\n",
    "    for i in sentiment :\n",
    "        if i == 1 :\n",
    "            word_sentiment.append('Pro')\n",
    "        elif i == 0 :\n",
    "            word_sentiment.append('Neutral')\n",
    "        elif i == -1 :\n",
    "            word_sentiment.append('Anti')\n",
    "        else :\n",
    "            word_sentiment.append('News')\n",
    "\n",
    "    df['sentiment'] = word_sentiment\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = update(train)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag extraction\n",
    "Hashtags are extracted from the original tweets and stored in seperate dataframes for each class. This is done before tweet cleaning to ensure no information is lost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_extract(tweet):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes in a tweet and extracts the top 15 hashtag(s) using regular expressions\n",
    "    These hashtags are stored in a seperate dataframe \n",
    "    along with a count of how frequenty they occur\n",
    "    \n",
    "    Input:\n",
    "    tweet: original tweets\n",
    "           datatype: 'str'\n",
    "           \n",
    "    Output:\n",
    "    hashtag_df: dataframe containing the top hashtags in the tweets\n",
    "              datatype: dataframe         \n",
    "    \"\"\"\n",
    "    \n",
    "    hashtags = []\n",
    "    \n",
    "    for i in tweet:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "        \n",
    "    hashtags = sum(hashtags, [])\n",
    "    frequency = nltk.FreqDist(hashtags)\n",
    "    \n",
    "    hashtag_df = pd.DataFrame({'hashtag': list(frequency.keys()),\n",
    "                       'count': list(frequency.values())})\n",
    "    hashtag_df = hashtag_df.nlargest(15, columns=\"count\")\n",
    "\n",
    "    return hashtag_df\n",
    "\n",
    "# Extracting the hashtags from tweets in each class\n",
    "pro = hashtag_extract(df['message'][df['sentiment'] == 'Pro'])\n",
    "anti = hashtag_extract(df['message'][df['sentiment'] == 'Anti'])\n",
    "neutral = hashtag_extract(df['message'][df['sentiment'] == 'Neutral'])\n",
    "news = hashtag_extract(df['message'][df['sentiment'] == 'News'])\n",
    "\n",
    "pro.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet cleaning\n",
    "Remove 'noisy entities' such as URL's, punctuations, mentions, numbers and extra white space. The data is further normalized by converting all letters to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetCleaner(tweet):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function uses regular expressions to remove url's, mentions, hashtags, \n",
    "    punctuation, numbers and any extra white space from tweets after converting \n",
    "    everything to lowercase letters.\n",
    "\n",
    "    Input:\n",
    "    tweet: original tweet\n",
    "           datatype: 'str'\n",
    "\n",
    "    Output:\n",
    "    tweet: modified tweet\n",
    "           datatype: 'str'\n",
    "    \"\"\"\n",
    "    # Convert everything to lowercase\n",
    "    tweet = tweet.lower() \n",
    "    \n",
    "    # Remove mentions   \n",
    "    tweet = re.sub('@[\\w]*','',tweet)  \n",
    "    \n",
    "    # Remove url's\n",
    "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r'#\\w*', '', tweet)    \n",
    "    \n",
    "    # Remove numbers\n",
    "    tweet = re.sub(r'\\d+', '', tweet)  \n",
    "    \n",
    "    # Remove punctuation\n",
    "    tweet = re.sub(r\"[,.;':@#?!\\&/$]+\\ *\", ' ', tweet)\n",
    "    \n",
    "    # Remove that funny diamond\n",
    "    tweet = re.sub(r\"U+FFFD \", ' ', tweet)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
    "    \n",
    "    # Remove space in front of tweet\n",
    "    tweet = tweet.lstrip(' ')                        \n",
    "    \n",
    "    return tweet\n",
    "\n",
    "# Clean the tweets in the message column\n",
    "df['message'] = df['message'].apply(TweetCleaner)\n",
    "df['message'] = df['message'].apply(TweetCleaner)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of speech tagging and lemmatization\n",
    "In this step we start by determining the length of each tweet and storing this information in a new column. We then tokenize the tweets before performing POS tagging on each word followed by lemmatization.\n",
    "\n",
    "In lemmatization, we reduce the word into dictionary root form. For instance \"cats\" is converted into \"cat\". Lemmatization is done in order to avoid creating features that are semantically similar but syntactically different. Lemmatization is preferred over stemming since stemming is a crude method for cataloging related words; it essentially chops off letters from the end until the stem is reached. This works fairly well in most cases, but unfortunately English has many exceptions where a more sophisticated process is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function modifies the original train dataframe.\n",
    "    A new column for the length of each tweet is added.\n",
    "    The tweets are then tokenized and each word is assigned a part of speech tag \n",
    "    before being lemmatized\n",
    "    \n",
    "    Input:\n",
    "    df: original dataframe\n",
    "        datatype: dataframe \n",
    "        \n",
    "    Output:\n",
    "    df: modified dataframe\n",
    "        datatype: dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df['length'] = df['message'].str.len()\n",
    "    df['tokenized'] = df['message'].apply(word_tokenize)\n",
    "    df['pos_tags'] = df['tokenized'].apply(nltk.tag.pos_tag)\n",
    "\n",
    "    def get_wordnet_pos(tag):\n",
    "\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "    \n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "        \n",
    "    wnl = WordNetLemmatizer()\n",
    "    df['pos_tags'] = df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "    df['lemmatized'] = df['pos_tags'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
    "    df['lemmatized'] = [' '.join(map(str, l)) for l in df['lemmatized']]  \n",
    "    return df\n",
    "\n",
    "df = lemma(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency¶\n",
    "Creating separate dataframes to store the 25 most frequent words and their respective frequencies for each class. Once this information has been extracted we will use these words to create wordclouds for each class.\n",
    "\n",
    "Word clouds are a popular approach in nlp tasks, here they help us visualize and gain a better understanding of what is being said in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(tweet):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function determines the frequency of each word in a collection of tweets \n",
    "    and stores the 25 most frequent words in a dataframe, \n",
    "    sorted from most to least frequent\n",
    "    \n",
    "    Input: \n",
    "    tweet: original tweets\n",
    "           datatype: 'str'\n",
    "           \n",
    "    Output: \n",
    "    frequency: dataframe containing the top 25 words \n",
    "               datatype: dataframe          \n",
    "    \"\"\"\n",
    "    \n",
    "    # Count vectorizer excluding english stopwords\n",
    "    cv = CountVectorizer(stop_words='english')\n",
    "    words = cv.fit_transform(tweet)\n",
    "    \n",
    "    # Count the words in the tweets and determine the frequency of each word\n",
    "    sum_words = words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create a dataframe to store the top 25 words and their frequencies\n",
    "    frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
    "    frequency = frequency.head(25)\n",
    "    \n",
    "    return frequency\n",
    "\n",
    "# Extract the top 25 words in each class\n",
    "pro_frequency = frequency(df['lemmatized'][df['sentiment']=='Pro'])\n",
    "anti_frequency = frequency(df['lemmatized'][df['sentiment']=='Anti'])\n",
    "news_frequency = frequency(df['lemmatized'][df['sentiment']=='News'])\n",
    "neutral_frequency = frequency(df['lemmatized'][df['sentiment']=='Neutral'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the words in the tweets for the pro and anti climate change classes \n",
    "anti_words = ' '.join([text for text in anti_frequency['word']])\n",
    "pro_words = ' '.join([text for text in pro_frequency['word']])\n",
    "news_words = ' '.join([text for text in news_frequency['word']])\n",
    "neutral_words = ' '.join([text for text in neutral_frequency['word']])\n",
    "\n",
    "# Create wordcloud for the anti climate change class\n",
    "anti_wordcloud = WordCloud(width=800, \n",
    "                           height=500, \n",
    "                           random_state=110, \n",
    "                           max_font_size=110, \n",
    "                           background_color='white',\n",
    "                           colormap=\"Reds\").generate(anti_words)\n",
    "\n",
    "# Create wordcolud for the pro climate change class\n",
    "pro_wordcloud = WordCloud(width=800, \n",
    "                          height=500, \n",
    "                          random_state=73, \n",
    "                          max_font_size=110, \n",
    "                          background_color='white',\n",
    "                          colormap=\"Greens\").generate(pro_words)\n",
    "\n",
    "# Create wordcolud for the news climate change class\n",
    "news_wordcloud = WordCloud(width=800, \n",
    "                          height=500, \n",
    "                          random_state=0, \n",
    "                          max_font_size=110, \n",
    "                          background_color='white',\n",
    "                          colormap=\"Blues\").generate(news_words)\n",
    "\n",
    "# Create wordcolud for the neutral climate change class\n",
    "neutral_wordcloud = WordCloud(width=800, \n",
    "                          height=500, \n",
    "                          random_state=10, \n",
    "                          max_font_size=110, \n",
    "                          background_color='white',\n",
    "                          colormap=\"Oranges\").generate(neutral_words)\n",
    "\n",
    "pro_frequency.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific named entity recognition and extraction¶\n",
    "Extracting the top 10 organisations, people and geopolitical entities in each class. This information is then stored in separate dataframes for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_extractor(tweet):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function extracts the top 10 people, organizations and geopolitical entities \n",
    "    in a collection of tweets. \n",
    "    The information is then saved in a new dataframe\n",
    "\n",
    "    Input:\n",
    "    tweet: lemmatized tweets\n",
    "           datatype: 'str'\n",
    "\n",
    "    Output:\n",
    "    df: dataframe containing the top 10 people, organizations and gpe's in a collection of tweets\n",
    "        datatype: dataframe ('str')\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_people(tweet):  \n",
    "        words = nlp(tweet)\n",
    "        people = [w.text for w in words.ents if w.label_== 'PERSON']\n",
    "        return people\n",
    "    \n",
    "    def get_org(tweet):\n",
    "        words = nlp(tweet)\n",
    "        org = [w.text for w in words.ents if w.label_== 'ORG']\n",
    "        return org\n",
    "    \n",
    "    def get_gpe(tweet):\n",
    "        words = nlp(tweet)\n",
    "        gpe = [w.text for w in words.ents if w.label_== 'GPE']\n",
    "        return gpe\n",
    "    \n",
    "    # Extract the top 10 people\n",
    "    people = tweet.apply(lambda x: get_people(x)) \n",
    "    people = [x for sub in people for x in sub]\n",
    "    people_counter = Counter(people)\n",
    "    people_count = people_counter.most_common(10)\n",
    "    people, people_count = map(list, zip(*people_count))\n",
    "    \n",
    "    # Extract the top 10 organizations\n",
    "    org = tweet.apply(lambda x: get_org(x)) \n",
    "    org = [x for sub in org for x in sub]\n",
    "    org_counter = Counter(org)\n",
    "    org_count = org_counter.most_common(10)\n",
    "    org, org_count = map(list, zip(*org_count))\n",
    "    \n",
    "    # Extract the top 10 geopolitical entities\n",
    "    gpe = tweet.apply(lambda x: get_gpe(x)) \n",
    "    gpe = [x for sub in gpe for x in sub]\n",
    "    gpe_counter = Counter(gpe)\n",
    "    gpe_count = gpe_counter.most_common(10)\n",
    "    gpe, gpe_count = map(list, zip(*gpe_count))\n",
    "    \n",
    "    # Create a dataframe to store the information\n",
    "    df = pd.DataFrame({'people' : people})\n",
    "    df['geopolitics'] = gpe\n",
    "    df['organizations'] = org\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Extract top entities for each class\n",
    "anti_info = entity_extractor(df['lemmatized'][df['sentiment']=='Anti'])\n",
    "pro_info = entity_extractor(df['lemmatized'][df['sentiment']=='Pro'])\n",
    "news_info = entity_extractor(df['lemmatized'][df['sentiment']=='News'])\n",
    "neutral_info = entity_extractor(df['lemmatized'][df['sentiment']=='Neutral'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "### Target variable distribution¶\n",
    "Taking a closer look at the distribution of the tweets we notice that the data is severely imbalanced with the majority of tweets falling in the 'pro' category, supporting the belief of man-made climate change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display target distribution\n",
    "style.use('seaborn-pastel')\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, \n",
    "                         nrows=1, \n",
    "                         figsize=(20, 10), \n",
    "                         dpi=100)\n",
    "\n",
    "sns.countplot(df['sentiment'], ax=axes[0])\n",
    "\n",
    "labels=['Pro', 'News', 'Neutral', 'Anti'] \n",
    "\n",
    "axes[1].pie(df['sentiment'].value_counts(),\n",
    "            labels=labels,\n",
    "            autopct='%1.0f%%',\n",
    "            shadow=True,\n",
    "            startangle=90,\n",
    "            explode = (0.1, 0.1, 0.1, 0.1))\n",
    "\n",
    "fig.suptitle('Tweet distribution', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet length distribution¶\n",
    "From the plots below we can see that tweets that fall in the pro climate change class are generally longer and the shortest tweets belong to the anti climate change class. We also notice that neutral climate change tweets tend to have the most variability in tweet length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the length tweets for each class using a box plot\n",
    "sns.boxplot(x=df['sentiment'], y=df['length'], data=df, palette=(\"Blues_d\"))\n",
    "plt.title('Tweet length for each class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climate change buzzwords\n",
    "The figures below display the 25 most common words found in the tweets for each classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pro and anti wordclouds next to one another for comparisson\n",
    "f, axarr = plt.subplots(2,2, figsize=(35,25))\n",
    "axarr[0,0].imshow(pro_wordcloud, interpolation=\"bilinear\")\n",
    "axarr[0,1].imshow(anti_wordcloud, interpolation=\"bilinear\")\n",
    "axarr[1,0].imshow(neutral_wordcloud, interpolation=\"bilinear\")\n",
    "axarr[1,1].imshow(news_wordcloud, interpolation=\"bilinear\")\n",
    "\n",
    "# Remove the ticks on the x and y axes\n",
    "for ax in f.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.axis('off')\n",
    "\n",
    "axarr[0,0].set_title('Pro climate change\\n', fontsize=35)\n",
    "axarr[0,1].set_title('Anti climate change\\n', fontsize=35)\n",
    "axarr[1,0].set_title('Neutral\\n', fontsize=35)\n",
    "axarr[1,1].set_title('News\\n', fontsize=35)\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Pro climate change buzzwords 20-25 shown here for clarity \\n- The wordcloud doesn't seem to pick up on 'http'\")\n",
    "display(pro_frequency.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- The top 3 buzzwords accross all classes are climate change and rt (retweet). The frequency of rt ( Retweet ) means that a lot of the same information and/or opinions are being shared and viewed by large audiences. This is true for all 4 classes\n",
    "\n",
    "- 'Trump' is a frequently occuring word in all 4 classes. This is unsurprising given his controversial view on the topic.\n",
    "\n",
    "- Words like real, believe, think, fight, etc. occur frequently in pro climate change tweets. In contrast, anti climate change tweets contain words such as 'hoax', 'scam', 'tax', 'liberal' and 'fake'. There is a stark difference in tone and use of emotive language in these 2 sets of tweets. From this data we could reason that people who are anti climate change believe that global warming is a 'hoax' and feel negatively towards a tax–based approach to slowing global climate change\n",
    "\n",
    "- words like 'science' and 'scientist' occur frequently as well which could imply that people are tweeting about scientific studies that support their views on climate change.\n",
    "\n",
    "- EPA, the United States Environmental Protection Agency is another climate change 'buzzword' that appears frequently across classes.\n",
    "\n",
    "- https occurs frequently in pro climate change tweets, implying that many links are being shared around the topic of climate change. These could be links to petitions, websites and/or articles related to climate change. Interesting to note: https only occurs in the top 25 words for the pro climate change class. Why aren't we seeing more links in the news class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtags\n",
    "Hashtags have long been an important tool on Twitter for helping users organize and sort their tweets. They're a great way to indicate that your content is relevant to a certain topic and to get your tweets in front of an interested audience.\n",
    "\n",
    "Considering this, we decided it might be insightful to see what the most frequent hashtags in each class are. This will help us gain a better understanding of what kind of information is being consumed and shared in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the frequent hastags for pro and anti climate change classes\n",
    "sns.barplot(data=pro,y=pro['hashtag'], x=pro['count'], palette=(\"Blues_d\"))\n",
    "plt.title('Frequent PRO climate change hashtags')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- One of the most popular hashtags used in pro climate change tweets is 'BeforeTheFlood' which refers to a 2016 documentary where actor Leonardo DiCaprio meets with scientists, activists and world leaders to discuss the dangers of climate change and possible solutions.\n",
    "\n",
    "- We also see a lot of ImVotingBecause and IVotedBecause hashtags in pro climate change tweets. For Democrats, climate change is now one of the two most important issues in politics, according to a new poll. Among all voters, the warming planet is now one of the most important issues in American politics. The poll was conducted by Climate Nexus, a nonpartisan nonprofit group, in partnership with researchers at Yale and George Mason University, and included nearly 2,000 registered voters.\n",
    "\n",
    "- COP22, ParisAgreement and Trump are the 5th, 6th and 8th most used hashtags in pro climate change tweets. Last year Trump’s administration formally began the process to exit the climate deal, in which nearly 200 nations pledged to reduce greenhouse gas emissions and assist poor nations struggling with the consequences of a warming Earth. Tweets that include these hashtags are most likely centered around peoples' opinions and criticism of Trump's decision and the implications thereof.\n",
    "\n",
    "- An interesting hashtag that made the top 15 is auspol which is short for Australian politics. Scientists have published the first assessment quantifying the role of climate change in the recent Australian bushfires. Global warming boosted the risk of the hot, dry weather that's likely to cause bushfires by at least 30%, they say. But the study suggests the figure is likely to be much greater. It says that if global temperatures rise by 2C, as seems likely, such conditions would occur at least four times more often. The analysis has been carried out by the World Weather Attribution consortium. Furthermore, Australia is ranked third in the world for climate change denial behind the United States and Sweden. Australian Prime Minister Scott Morrison, who has resisted calls for the country to reduce its carbon emissions, has been accused of deemphasizing the the link between the bushfires and climate change, saying during a 2019 November interview that there isn’t “credible scientific evidence” that curbing emissions would diminish the fires. Tweets including this hashtag could be trying to raise awareness about the link between a warming earth and bushfires in Australia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=anti,y=anti['hashtag'], x=anti['count'], palette=(\"Blues_d\"))\n",
    "plt.title('Frequent ANTI climate change hashtags')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- MAGA and Trump are the number 1 and 3rd most frequently used hashtags in anti climate change tweets. MAGA (Make America great again) is the slogan that Donald Trump used during his campaign for elections in 2016. On Twitter, his supporters took to the hashtag “#MAGA.” The MAGA hat and hashtag became, and remain, symbols of support for Trump. From this information we can infer that most anti climate change tweets come from Trump suporters.\n",
    "\n",
    "- DrainTheSwamp also made the list of the top anti climate change hashtags. Trump used this metaphor to describe his plan to fix problems in the federal government. During his presidential campaign, Trump claimed then that he would raise taxes on the wealthy, particularly the hedge fund managers. This is another hashtag implying support for trump which futher supports our assumtion that most trump suporters on twitter also fall in the anti climate change category.\n",
    "\n",
    "- TCOT which stands for Top Conservative On Twitter takes the number 6 spot. The term provides a way for conservatives in particular and Republicans in general to locate and follow the tweets of like-minded individuals. We're sensing a pattern here: Trump, top conservatives on twitter, make America great again, Drain the swamp...\n",
    "\n",
    "- FakeNews and ClimateScam are pretty popular hashtags too. People who are anti climate change could be tweeting and retweeting information and opinions they disagree with followed by the hashtags 'fakenews' and 'climatescam' in an attemot to discredit both the information and the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the frequent hastags for the news and neutral classes\n",
    "sns.barplot(y=news['hashtag'], x=news['count'], palette=(\"Blues_d\"))\n",
    "plt.title('Frequent climate change NEWS hashtags')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "The hashtags in the news category are less emotive and aim to bring awareness to high profile topics related to climate change that are or were trending in the news. Examples of such hashtags include News and Science which would be used to indicate that the tweet contains information from a news outlet or a scientific study.\n",
    "\n",
    "GreatBarrierReef, Environment and ClimateChange are more examples of hashtags that are more aimed at drawing attention to and sharing information around climate change\n",
    "\n",
    "ParisAgreement, COP22 and Trump are popular hashtags. Trump made headlines when he pulled out of the climate agreement, so it makes sense that these hashtags would be trending in climate change news.\n",
    "\n",
    "ClimateMarch - many protests have been held, some even global, to raise awareness about climate change. These protests usually make headlines and are featured on news sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y=neutral['hashtag'], x=neutral['count'], palette=(\"Blues_d\"))\n",
    "plt.title('Frequent NEUTRAL climate change hashtags')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Observations:\n",
    "In the neutral category, a few of the top hashtags were not directly related to climate change eg. FirstDayOfSpring and AprilFoolsDay.\n",
    "Hashtags like GlobalWarming, AmReading and QandA could suggest that these tweets are made by people who are still undecided on the topic but could be open to discussions and are interested in finding new/more information.\n",
    "In general neutral climate change tweets have hashtags that are not as polarized as the anti and pro hashtags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People, places and organizations to watch\n",
    "We performed NER on the train data in order to determine the most mentioned people, organizations and geopolitical entities in each class. This lead us to some interesting insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pro climate change information')\n",
    "display(pro_info.head(9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "Donald Trump is the top mentioned person in pro climate change tweets. This could be because many climate change activists and people who feel strongly about climate change have a lot to say about Trump and his opinions regarding climate change. People tend to tweet more when it comes to things they are unhappy about, want to change or create awareness about.\n",
    "\n",
    "Another top mentioned person is Edward Scott Pruitt, an American lawyer, lobbyist and Republican politician. Pruitt served as the Administrator of the Environmental Protection Agency (EPA) in 2017 which is the top mentioned organization in pro climate change tweets.\n",
    "\n",
    "The GOP is the 4th most mentioned organization which is a nickname for the the Republican Party. This could be because Republicans have become less convinced over time that the effects of pollution from human activities are the cause of climate change. In a 2019 Gallup poll, 89% of Democrats compared to 34% of Republicans said they believe increases in the Earth’s temperature are due more to the effects of pollution from human activities than because of natural changes in the environment. These tweets mentioning the republican party could in fact be criticism from Democrats.\n",
    "\n",
    "The 5th most mentioned organization is Exxon, an oil and natural resource company. Among the oil spills that occurred in the last five decades, Exxon Valdez Oil Spill remains a prominent one. In the accident that took place almost 30 years ago, over 11 million gallons of crude oil was released into the waters of the Gulf of Alaska, hurting the ecosystem badly as it killed hundreds of thousands of species. Exxon has previously been under investigation for potential fraud by withholding information on the role of fossil fuels in driving up temperatures. Considering this information, it comes as no surprise that Exxon is trending in pro climate change tweets for all the wrong reasons.\n",
    "\n",
    "The natural resource defense council and united nations are also frequently mentioned organizations in pro climate change tweets. The NRDC is a non-profit international environmental advocacy group using law, science and the support of more than 2 million members and online activists to protect the planet's wildlife.\n",
    "\n",
    "America and China are responsible for 40% of the world's carbon emissions and are the most mentioned geopolitical entities in pro climate change tweets, most likely for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Anti climate change information')\n",
    "display(anti_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "Interestingly enough, Trump does not feature in the top 10 mentioned people in anti climate change tweets, instead the most mentioned person is Al Gore, American politician, environmentalist and former vice president. Al Gore is pro climate change and vocal about it, having published a number of books on the topic. It's possible that his publications are a popular topic of discussion and his ideas are being critistized by the anti climate change community.\n",
    "\n",
    "Former Trump adviser, Steve Bannon is the 5th most mentioned person in anti climate change tweets. Stephen Bannon has called government support of alternative energy \"madness.\" His conservative website, Breitbart News, relentlessly pursues the idea that global warming is an invention of activists, university researchers and renewable energy industry profiteers determined to assert global governance for their own gain. Anti climate change tweets could be sharing and retweeting a lot of his views and stories from his website.\n",
    "\n",
    "Golden eagle medium claim fictional - Thousands of golden eagles have been killed by wind turbines. A 2013 study published in The Wildlife Society Bulletin found that wind turbines killed an estimated 573,000 birds annually in the United States. And that figure was 7 years ago. Some of these anti climate change tweets could be raising concern around wind turbines and the danger they pose to these birds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building classification models\n",
    "We will be making use of a pipeline to build our classification models. This pipeline will vectorize the text data before fitting it to our chosen model.\n",
    "\n",
    "The following 5 models will be considered:\n",
    "\n",
    "- Random forest\n",
    "- Naive Bayes\n",
    "- K nearest neighbors\n",
    "- Logistic regression\n",
    "- Linear SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train - Validation split\n",
    "Before we pass our data through our custom pipelines we have to split our train data into features and target variables. After this step we can split our train data into a train and validation set. This will allow us to evaluate our model performance and chose the best model to use for our submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train & validation (25%) for model training\n",
    "\n",
    "# Seperate features and tagret variables\n",
    "X = train['message']\n",
    "y = train['sentiment']\n",
    "\n",
    "# Split the train data to create validation dataset\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines\n",
    "The pipelines consist of 2 steps, vectorization and model fitting.\n",
    "\n",
    "Machines, unlike humans, cannot understand the raw text. Machines can only see numbers. Particularly, statistical techniques such as machine learning can only deal with numbers. Therefore, we need to convert our text into numbers.\n",
    "\n",
    "The TFIDF vectorizer assigns word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents. The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Another advantage of this method is that the resulting vectors are already scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "               ('clf', RandomForestClassifier(max_depth=5, \n",
    "                                              n_estimators=100))])\n",
    "\n",
    "# Naïve Bayes:\n",
    "nb = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "               ('clf', MultinomialNB())])\n",
    "\n",
    "# K-NN Classifier\n",
    "knn = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                ('clf', KNeighborsClassifier(n_neighbors=5, \n",
    "                                             metric='minkowski', \n",
    "                                             p=2))])\n",
    "\n",
    "# Logistic Regression\n",
    "lr = Pipeline([('tfidf',TfidfVectorizer()),\n",
    "               ('clf',LogisticRegression(C=1, \n",
    "                                         class_weight='balanced', \n",
    "                                         max_iter=1000))])\n",
    "# Linear SVC:\n",
    "lsvc = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                 ('clf', LinearSVC(class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the models\n",
    "The models are trained by passing the train data through each custom pipeline. The trained models are then used to predict the classes for the validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest \n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_valid)\n",
    "\n",
    "# Niave bayes\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_valid)\n",
    "\n",
    "# K - nearest neighbors\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_valid)\n",
    "\n",
    "# Linear regression\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_valid)\n",
    "\n",
    "# Linear SVC\n",
    "lsvc.fit(X_train, y_train)\n",
    "y_pred_lsvc = lsvc.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model evaluation\n",
    "The performance of each model will be evaluated based on the precision, accuracy and F1 score achieved when the model is used to predict the classes for the validation data. We will be looking at the following to determine and visualize these metrics:\n",
    "\n",
    "Classification report\n",
    "Confusion matrix\n",
    "The best model will be selected based on the weighted F1 score.\n",
    "\n",
    "Random forest classification\n",
    "Random Forest is a tree-based machine learning algorithm that leverages the power of multiple decision trees for making decisions. As the name suggests, it is a “forest” of trees!\n",
    "\n",
    "The following diagram is a visual representation of the random forest classification method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification Report for the random forest model\n",
    "print(metrics.classification_report(y_valid, y_pred_rf))\n",
    "\n",
    "# Generate a normalized confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_rf)\n",
    "cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "sns.heatmap(cm_norm, \n",
    "            cmap=\"YlGnBu\", \n",
    "            xticklabels=rf.classes_, \n",
    "            yticklabels=rf.classes_, \n",
    "            vmin=0., \n",
    "            vmax=1., \n",
    "            annot=True, \n",
    "            annot_kws={'size':10})\n",
    "\n",
    "# Adding headings and lables\n",
    "plt.title('Random forest classification')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Observations:\n",
    "From the confusion matrix above we notice that the random forest classification model does a very poor job on our data set. The model classifies all the tweets as pro climate change tweets.\n",
    "This results in precision, recall and F1 scores of zero for the anti, neutral and news classes.\n",
    "Tree based classification models are especially vulnerable to overfitting when the train data is imbalanced which is the case with our data. The model could be greatly improved by using resampling techniques such as oversampling the anti class and/or undersampling the pro class. This will allow the model to learn how to classify each class equally, improving its accuracy.\n",
    "The overall F1 score is 0.55. This is a relatively high score for a model that simply classifies all tweets into a single class. This score could only be achieved since the majority of the tweets are in fact pro climate change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive Bayes classification\n",
    "Naive Bayes is a classification algorithm that uses the principle of Bayes theorem to make classifications and assumes independent variables to be statistically independent from each other .\n",
    "\n",
    "Bayes Theorem:\n",
    "\n",
    "P(A∣B)=P(B∣A)∗P(A)P(B)\n",
    " \n",
    "P(A|B) is the posterior probability of class (A, target) given predictor (B, attributes).\n",
    "\n",
    "P(A) is the prior probability of class.\n",
    "\n",
    "P(B|A) is the likelihood which is the probability of the predictor given class.\n",
    "\n",
    "P(B) is the prior probability of the predictor.\n",
    "\n",
    "Naive Bayes has 3 Classification Methods\n",
    "\n",
    "Gaussian : It is used in classification and it assumes that features follow a normal distribution.\n",
    "\n",
    "Bernoulli : The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with a ‘bag of words’ model where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively.\n",
    "\n",
    "Multinomial : It is used for discrete counts. For example, let’s say, we have a text classification problem. Here we can consider Bernoulli trials which is one step further and instead of “word occurring in the document”, we have “count how often word occurs in the document”, you can think of it as “number of times outcome number x_i is observed over the n trials”.\n",
    "\n",
    "We will be using the multinomial method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification Report for the Naive Bayes model\n",
    "print(metrics.classification_report(y_valid, y_pred_nb))\n",
    "\n",
    "# Generate a normalized confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_nb)\n",
    "cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "sns.heatmap(cm_norm, \n",
    "            cmap=\"YlGnBu\", \n",
    "            xticklabels=nb.classes_, \n",
    "            yticklabels=nb.classes_, \n",
    "            vmin=0., \n",
    "            vmax=1., \n",
    "            annot=True, \n",
    "            annot_kws={'size':10})\n",
    "\n",
    "# Adding headings and lables\n",
    "plt.title('Naive Bayes classification')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Observations:\n",
    "Although the Naive Bayes model is a slight improvement on the random forest model it still performs poorly\n",
    "This model classifies most tweets as pro climate change with improved predictions for the news class.\n",
    "The precision, accuracy and F1 scores have improved significantly for the news class but remain low for neutral and anti.\n",
    "The overall F1 score is 0.63. Again this score could only be achieved since the majority of tweets are in fact pro climate change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K Nearest Neighbors classification\n",
    "The KNN algorithm uses ‘feature similarity’ to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set.\n",
    "\n",
    "There are various methods for calculating how closely the new datapoint resembles the points in the training set, of which the most commonly known methods are – Euclidian, Manhattan (for continuous) and Hamming distance (for categorical).\n",
    "\n",
    "New datapoint( Ci ) and multi-classes {1,2,3}.\n",
    "\n",
    "KNN Classifier would use one of the distance criteria to classify the new datapoint class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification Report for the K-nearest neighbors model\n",
    "print(metrics.classification_report(y_valid, y_pred_knn))\n",
    "\n",
    "# Generate a normalized confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_knn)\n",
    "cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "sns.heatmap(cm_norm, \n",
    "            cmap=\"YlGnBu\", \n",
    "            xticklabels=knn.classes_, \n",
    "            yticklabels=knn.classes_, \n",
    "            vmin=0., \n",
    "            vmax=1., \n",
    "            annot=True, \n",
    "            annot_kws={'size':10})\n",
    "\n",
    "# Adding headings and lables\n",
    "plt.title('K - nearest neighbors classification')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Observations:\n",
    "KNN is able to successfully classify the tweets.\n",
    "This model also classifies most tweets as pro climate change but to a smaller degree compared to the previous 2 models.\n",
    "The precision, accuracy and F1 scores have improved significantly for the pro, anti and neutral classes.\n",
    "There is a drop in the F1 score for the pro climate change class as the predictions become more balanced.\n",
    "The overall F1 score is 0.69 which is very close to our target but not there yet... The search continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic regression classification\n",
    "Logistic Regression uses the probability of a data point to belonging to a certain class to classify each datapoint to it's best estimated class\n",
    "\n",
    "Logistic regression has been rated as the best performing model for linearly separable data especially if it's predicting binary data(Yes & NO or 1 & 0), and performs better when there's no class imbalance.\n",
    "\n",
    "The figure below is the sigmoid function logistic regression models use to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification Report for the model\n",
    "print(metrics.classification_report(y_valid, y_pred_lr))\n",
    "\n",
    "cm = confusion_matrix(y_valid, y_pred_lr)\n",
    "cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "sns.heatmap(cm_norm, \n",
    "            cmap=\"YlGnBu\", \n",
    "            xticklabels=lr.classes_, \n",
    "            yticklabels=lr.classes_, \n",
    "            vmin=0., \n",
    "            vmax=1., \n",
    "            annot=True, \n",
    "            annot_kws={'size':10})\n",
    "\n",
    "# Adding headings and lables\n",
    "plt.title('Logistic regression classification')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Observations:\n",
    "Logistic regression is able to successfully classify the tweets.\n",
    "This model classifies most tweets successfully with clear boundaries and less confusion surrounding the pro climate change class.\n",
    "The precision, accuracy and F1 scores have improved significantly for the pro, anti and neutral classes.\n",
    "There is a drop in the F1 score for the pro climate change class as the predictions become more balanced.\n",
    "The overall F1 score is 0.71 which is on target. Let's see if we can improve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear SVC classification\n",
    "In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. The goal of the SVM algorithm is to create the best line or decision boundary that can seperate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.\n",
    "\n",
    "SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called as support vectors. Consider the below diagram in which there are two different categories that are classified using a decision boundary or hyperplane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification Report for the linear SVC model\n",
    "print(metrics.classification_report(y_valid, y_pred_lsvc))\n",
    "\n",
    "# Generate a normalized confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_lsvc)\n",
    "cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "sns.heatmap(cm_norm, \n",
    "            cmap=\"YlGnBu\", \n",
    "            xticklabels=lsvc.classes_, \n",
    "            yticklabels=lsvc.classes_, \n",
    "            vmin=0., \n",
    "            vmax=1., \n",
    "            annot=True, \n",
    "            annot_kws={'size':10})\n",
    "\n",
    "# Adding headings and lables\n",
    "plt.title('Linear SVC classification')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Observations:\n",
    "Linear SVC is able to successfully classify the tweets.\n",
    "This model classifies most tweets successfully with clear boundaries and less confusion surrounding the pro climate change class compared to the first 3 models.\n",
    "This model shows a higher degree of confusion surrounding the pro class compared to logistic regression.\n",
    "This, however, leads to an increase in the precision, accuracy and f1 score for the pro class which makes up the majority of the tweets.\n",
    "Linear SVC has achieved the highest F1 score of 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model Selection\n",
    "Linear SVC has achieved the highest F1 score of 0.75 and is therefore our model of choice moving forward.\n",
    "\n",
    "Hyperparameter tuning\n",
    "Once our top performing model has been selected, we attempt to improve it by performing some hyperparameter tuning.\n",
    "\n",
    "After the optimal parameters are determined the linear SVC model is retrained using these parameters, resulting in a 2% increase in the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is intentionally commented out - Code takes >10 minutes to run. \n",
    "\n",
    "\"\"\"\n",
    "# Set ranges for the parameters that we want to tune\n",
    "params = {'clf__C': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "          'tfidf__ngram_range' : [(1,1),(1,2)],\n",
    "          'clf__max_iter': [1500, 2000, 2500, 3000],\n",
    "          'tfidf__min_df': [2, 3, 4],\n",
    "          'tfidf__max_df': [0.8, 0.9]}\n",
    "\n",
    "# Perform randomized search & extract the optimal parameters\n",
    "Randomized = RandomizedSearchCV(text_clf_lsvc, param_distributions=params, cv=5, scoring='accuracy', n_iter=5, random_state=42)\n",
    "Randomized.fit(X_train,y_train)\n",
    "Randomized.best_estimator_\n",
    "\"\"\"\n",
    "\n",
    "# Retrain linear SVC using optimal hyperparameters:\n",
    "lsvc_op = Pipeline([('tfidf', TfidfVectorizer(max_df=0.8,\n",
    "                                                    min_df=2,\n",
    "                                                    ngram_range=(1,2))),\n",
    "                  ('clf', LinearSVC(C=0.3,\n",
    "                                    class_weight='balanced',\n",
    "                                    max_iter=3000))])\n",
    "\n",
    "# Fit and predict\n",
    "lsvc_op.fit(X_train, y_train)\n",
    "y_pred = lsvc_op.predict(X_valid)\n",
    "\n",
    "print('F1 score improved by',\n",
    "      round(100*((metrics.accuracy_score(y_pred, y_valid) - metrics.accuracy_score(y_pred_lsvc, y_valid)) /metrics.accuracy_score(y_pred_lsvc, y_valid)),0), \n",
    "      '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Comet experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Saving each metric to add to a dictionary for logging\n",
    "f1 = f1_score(y_valid, y_pred, average='weighted')\n",
    "precision = precision_score(y_valid, y_pred, average='weighted')\n",
    "recall = recall_score(y_valid, y_pred, average='weighted')\n",
    "\n",
    "# Create dictionaries for the data we want to log          \n",
    "metrics = {\"f1\": f1,\n",
    "           \"recall\": recall,\n",
    "           \"precision\": precision}\n",
    "\n",
    "params= {'classifier': 'linear SVC',\n",
    "         'max_df': 0.8,\n",
    "         'min_df': 2,\n",
    "         'ngram_range': '(1,2)',\n",
    "         'vectorizer': 'Tfidf',\n",
    "         'scaling': 'no',\n",
    "         'resampling': 'no',\n",
    "         'test_train random state': '0'}\n",
    "  \n",
    "# Log info on comet\n",
    "experiment.log_metrics(metrics)\n",
    "experiment.log_parameters(params)\n",
    "\n",
    "# End experiment\n",
    "experiment.end()\n",
    "\n",
    "# Display results on comet page\n",
    "experiment.display()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/climate-change-belief-analysis/test.csv')\n",
    "y_test = lsvc_op.predict(test['message'])\n",
    "output = pd.DataFrame({'tweetid': test.tweetid,\n",
    "                       'sentiment': y_test})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "More than half of the tweets examined support the beilief of man-made climate change. Futhermore, climate change is now one of the two most important issues in politics for Democrats. The data also suggests that the majority of anti climate change tweets come from Republicans and Trump supporters\n",
    "\n",
    "Last year Trump’s administration formally began the process to exit the climate deal, in which nearly 200 nations pledged to reduce greenhouse gas emissions and assist poor nations struggling with the consequences of a warming Earth. We noticed that the majority of tweets about climate change accross all classes involve the Paris agreement, COP22, Trump and Trump related hashtags/mentions. It was intresting to note that the most links are being shared in the pro climate change class and not in the news related class.\n",
    "\n",
    "Our final kaggle subission made use of a tuned linear SVC model and achived an F1 score of 0.74.\n",
    "\n",
    "For further information regarding the possible business applications of these insights and as well as access to our interactive classification model and data visualizations please visit our streamlit app:\n",
    "\n",
    "Link: http://54.194.109.116:5000/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
