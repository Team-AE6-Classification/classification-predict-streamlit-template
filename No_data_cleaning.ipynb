{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "solar-battle",
   "metadata": {},
   "source": [
    "# Classification Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-roller",
   "metadata": {},
   "source": [
    "### Climate change tweet classsification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-cannon",
   "metadata": {},
   "source": [
    "This notebook is aimed at classifying if a person/custumer believe in climate change or not, this is to help companies with determining if customising their product for lessining carbon footprint and environmental impact, is a good idea or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-attention",
   "metadata": {},
   "source": [
    "## Importing the modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-removal",
   "metadata": {},
   "source": [
    "Start by importing packages that will be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "colored-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# set plot style\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nlppreprocess import NLP\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-tanzania",
   "metadata": {},
   "source": [
    "Checking the fields of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "excited-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test_with_no_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-argentina",
   "metadata": {},
   "source": [
    "## Text to numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-outreach",
   "metadata": {},
   "source": [
    "Since the model only recognise numeric data for predictions, the text data has to be converted to numeric data type and this is achieved using text frequency - inverse document frequency (TF-IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "czech-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lasting-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF for coverting text to numeric data type\n",
    "data = train_df['message']\n",
    "\n",
    "vectorizer=TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "neither-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_df['message']\n",
    "\n",
    "vectorized_t = vectorizer.transform(test_data)\n",
    "X_test_t = vectorized_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "related-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arranging the data into predictor variables and label\n",
    "X = X_vectorized\n",
    "y = train_df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-sussex",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-surveillance",
   "metadata": {},
   "source": [
    "#### NB: Splitting data in this case is just for checking the classification report  but the submissions on kaggle are predicted without splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-progressive",
   "metadata": {},
   "source": [
    "Spliting the training data into test and training, or test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "complimentary-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the training data into 95% training and 5% training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-samuel",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-respect",
   "metadata": {},
   "source": [
    "#### hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "solvers = ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,C=c_values)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, scoring='f1')\n",
    "grid_result = grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-period",
   "metadata": {},
   "source": [
    "#### Checking the F1 score and classification report of the split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "auburn-developer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score:  0.76\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.70      0.51      0.59        61\n",
      "           0       0.61      0.47      0.53       123\n",
      "           1       0.80      0.86      0.83       441\n",
      "           2       0.79      0.84      0.82       166\n",
      "\n",
      "    accuracy                           0.77       791\n",
      "   macro avg       0.73      0.67      0.69       791\n",
      "weighted avg       0.76      0.77      0.76       791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C= 100, solver = 'liblinear', random_state =42)\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print('F1_score: ',round(metrics.f1_score(y_test,y_pred, average = 'weighted'),3))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\", metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-while",
   "metadata": {},
   "source": [
    "#### making prediction for unsplit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "popular-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X,y)\n",
    "y_pred_t = lr.predict(X_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-accountability",
   "metadata": {},
   "source": [
    "## Saving the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-darwin",
   "metadata": {},
   "source": [
    "Tunning the predictions into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "spectacular-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'tweetid':test_df['tweetid'],\n",
    "                          'sentiment':y_pred_t})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-valuation",
   "metadata": {},
   "source": [
    "copying the predictions into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('classification6.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-strip",
   "metadata": {},
   "source": [
    "Use the above saved CSV file on Kaggle for submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-inspection",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_save_path = \"lr25.pkl\"\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(lr,file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
